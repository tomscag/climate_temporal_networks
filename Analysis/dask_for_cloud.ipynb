{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask.distributed import LocalCluster, Client\n",
    "import dask\n",
    "import numpy as np\n",
    "import os\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import webbrowser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosscorrelation(x, y, maxlag, normalize=True):\n",
    "    \"\"\"\n",
    "    Cross correlation with a maximum number of lags, with optional normalization.\n",
    "\n",
    "    Parameters: \n",
    "    x, y: one-dimensional numpy arrays with the same length.\n",
    "    maxlag: maximum lag for which the cross correlation is computed.\n",
    "    normalize: if True, calculate the normalized cross-correlation.\n",
    "    \n",
    "    Returns:\n",
    "    An array of cross-correlation values with length 2*maxlag + 1.\n",
    "    \n",
    "    Credits: \n",
    "    https://stackoverflow.com/questions/30677241/how-to-limit-cross-correlation-window-width-in-numpy\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    py = np.pad(y.conj(), 2*maxlag, mode='constant')\n",
    "    T = as_strided(py[2*maxlag:], shape=(2*maxlag+1, len(y) + 2*maxlag),\n",
    "                   strides=(-py.strides[0], py.strides[0]))\n",
    "    px = np.pad(x, maxlag, mode='constant')\n",
    "\n",
    "    \n",
    "    cross_corr = T.dot(px)\n",
    "\n",
    "    cmax = float(cross_corr.max())\n",
    "    the_lagmax = cross_corr.argmax() - (maxlag + 1)\n",
    "    \n",
    "    if normalize:\n",
    "        mx = np.mean(x)\n",
    "        my = np.mean(y)\n",
    "        norm = float(len(y) * mx * my)\n",
    "\n",
    "        numerator = cmax - norm\n",
    "\n",
    "        sigma_x = np.sqrt(np.sum((x - mx) ** 2))\n",
    "        sigma_y = np.sqrt(np.sum((y - my) ** 2))\n",
    "        denominator = float(sigma_x * sigma_y)\n",
    "        \n",
    "        cmax = numerator / denominator\n",
    "\n",
    "    return cmax, the_lagmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_chunk(data_chunk_subset, nodes, max_lag, year, outpath):\n",
    "    print(data_chunk_subset)\n",
    "    print(nodes)\n",
    "    print(max_lag)\n",
    "    \n",
    "    arrays = []\n",
    "    \n",
    "    for i in range(data_chunk_subset.dims['num']):\n",
    "\n",
    "        num_chunks = data_chunk_subset['t2m'].isel(num=i)\n",
    "        numpy_array = np.zeros((37*72, 37*72))  \n",
    "        \n",
    "        for indi, nod in enumerate(nodes):\n",
    "            Ai, Aj = nodes[indi]\n",
    "            for indj, node in enumerate(nodes):\n",
    "                (Bi, Bj) = nodes[indj]\n",
    "\n",
    "                if indi < indj:\n",
    "                    ts1 = num_chunks.isel(lat=Ai, lon=Aj).values\n",
    "                    ts2 = num_chunks.isel(lat=Bi, lon=Bj).values\n",
    "                    #crossmax = np.corrcoef(ts1, ts2)[0, 1]\n",
    "                    cmax, the_lagmax = crosscorrelation(ts1, ts2, 150)\n",
    "                    numpy_array[indi, indj] = cmax\n",
    "\n",
    "        arrays.append(numpy_array)\n",
    "\n",
    "    # Unisci gli array lungo un nuovo asse\n",
    "    result_array = np.stack(arrays, axis=0)\n",
    "     \n",
    "    medie = result_array[1:, :, :].mean(axis=0)\n",
    "    deviazioni_standard = result_array[1:, :, :].std(axis=0)\n",
    "\n",
    "   \n",
    "    z_scores = abs(result_array[0, :, :] - medie) / deviazioni_standard\n",
    "\n",
    "    np.save(f'{outpath}/zscores_npy_year{year}.npy', z_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ### DASK Client\n",
    "\n",
    "    cluster = LocalCluster(n_workers=15)     #  , memory_limit='4GB'\n",
    "    client = Client(cluster)\n",
    "    # Se non specifichi memory_limit, Dask divide la memoria disponibile del sistema equamente tra i worker.\n",
    "    print(f\"Dask Dashboard: {client.dashboard_link}\")   # Stampa il link per accedere alla dashboard\n",
    "\n",
    "\n",
    "    ### Dataset\n",
    "    original_ds = xr.open_dataset('X:\\surr_IAAFT_t2m_2022_2100_highemission.nc')\n",
    "\n",
    "\n",
    "    ### Parameters\n",
    "    grouped_ds = original_ds.groupby('time.year')\n",
    "    max_lag = 150\n",
    "    outpath = \"X:/zscore t2m_2022_2100_highemission\"\n",
    "    if not os.path.exists(outpath):\n",
    "        os.makedirs(outpath)\n",
    "    lon_range = range(0, len(original_ds['lon']))\n",
    "    lat_range = range(0, len(original_ds['lat']))\n",
    "    nodes = tuple((i,j) for i in lat_range for j in lon_range)\n",
    "\n",
    "    ### Make the nodelist\n",
    "    with open(f\"{outpath}/nodelist.txt\", \"w\") as f:\n",
    "        for idx, (lat_idx, lon_idx) in enumerate(nodes):\n",
    "            lat = original_ds['lat'].values[lat_idx]\n",
    "            lon = original_ds['lon'].values[lon_idx]\n",
    "            f.write(f\"{idx} {lat} {lon}\\n\")\n",
    "    \n",
    "\n",
    "    ### Analysis\n",
    "\n",
    "    delayed_results = []       # Lista per memorizzare i risultati ritardati\n",
    "\n",
    "    # Itera su ogni anno e crea un task ritardato\n",
    "    for year, data_chunk in grouped_ds:\n",
    "        data_chunk_subset = data_chunk.isel(num=slice(0, 31))  # seleziona surrogati da 0 a 11\n",
    "        delayed_results.append(dask.delayed(analyze_chunk)(data_chunk_subset, nodes, max_lag, year, outpath))\n",
    "        \n",
    "    # Visualizzare il grafo di esecuzione\n",
    "    dask.visualize(*delayed_results, optimize_graph=True, filename='my_graph.svg')\n",
    "    graph_path = os.path.abspath('my_graph.svg')\n",
    "    webbrowser.open(f'file://{graph_path}')\n",
    "\n",
    "    # Dask compute \n",
    "    computed_results = dask.compute(*delayed_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_ds = xr.open_dataset('X:\\surr_IAAFT_t2m_2022_2100_highemission.nc')\n",
    "grouped_ds = original_ds.groupby('time.year')\n",
    "grouped_ds\n",
    "\n",
    "max_lag = 150\n",
    "outpath = \"X:/zscore t2m_2022_2100_highemission\"\n",
    "if not os.path.exists(outpath):\n",
    "    os.makedirs(outpath)\n",
    "lon_range = range(0, len(original_ds['lon']))\n",
    "lon_range = range(20, 21)\n",
    "#lat_range = range(0, len(original_ds['lat']))\n",
    "lat_range = range(0, 10)\n",
    "nodes = tuple((i,j) for i in lat_range for j in lon_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, data_chunk in grouped_ds:\n",
    "    print(year)\n",
    "    data_chunk_subset = data_chunk.isel(num=slice(0, 31  ))  # seleziona surrogati da 0 a 11\n",
    "    #print(data_chunk_subset)\n",
    "    analyze_chunk(data_chunk_subset, nodes, max_lag, year, outpath) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD METHOD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask.distributed import LocalCluster, Client\n",
    "import dask\n",
    "\n",
    "# Avviare il client Dask\n",
    "cluster = LocalCluster(n_workers=24)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import xarray as xr\n",
    "\n",
    "def _check_arg(x, xname):\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError('%s must be one-dimensional.' % xname)\n",
    "    return x\n",
    "\n",
    "def crosscorrelationNORMALIZZAZIONE_ERRATA(x, y, maxlag, normalize=True):\n",
    "    \"\"\"\n",
    "    Cross correlation with a maximum number of lags.\n",
    "\n",
    "    `x` and `y` must be one-dimensional numpy arrays with the same length.\n",
    "\n",
    "    This computes the same result as\n",
    "        numpy.correlate(x, y, mode='full')[len(a)-maxlag-1:len(a)+maxlag]\n",
    "\n",
    "    The return vaue has length 2*maxlag + 1.\n",
    "\n",
    "    https://stackoverflow.com/questions/30677241/how-to-limit-cross-correlation-window-width-in-numpy\n",
    "    \"\"\"\n",
    "\n",
    "    if normalize:\n",
    "        x = (x - np.mean(x)) / (np.std(x))\n",
    "        y = (y - np.mean(y)) /  (np.std(y) )        \n",
    "\n",
    "    x = _check_arg(x, 'x')\n",
    "    y = _check_arg(y, 'y')\n",
    "    py = np.pad(y.conj(), 2*maxlag, mode='constant')\n",
    "    T = as_strided(py[2*maxlag:], shape=(2*maxlag+1, len(y) + 2*maxlag),\n",
    "                   strides=(-py.strides[0], py.strides[0]))\n",
    "    px = np.pad(x, maxlag, mode='constant')\n",
    "\n",
    "    \n",
    "    crossc = T.dot(px)\n",
    "\n",
    "    if normalize:\n",
    "        for lag in range(-maxlag,maxlag+1):\n",
    "            crossc[lag+maxlag] = crossc[lag+maxlag]/( len(x)-abs(lag) )\n",
    "\n",
    "    return crossc\n",
    "\n",
    "\n",
    "def crosscorrelation(x, y, maxlag, normalize=True):\n",
    "    \"\"\"\n",
    "    Cross correlation with a maximum number of lags, with optional normalization.\n",
    "\n",
    "    Parameters:\n",
    "    x, y: one-dimensional numpy arrays with the same length.\n",
    "    maxlag: maximum lag for which the cross correlation is computed.\n",
    "    normalize: if True, calculate the normalized cross-correlation.\n",
    "    \n",
    "    Returns:\n",
    "    An array of cross-correlation values with length 2*maxlag + 1.\n",
    "    \n",
    "    Credits: \n",
    "    https://stackoverflow.com/questions/30677241/how-to-limit-cross-correlation-window-width-in-numpy\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    py = np.pad(y.conj(), 2*maxlag, mode='constant')\n",
    "    T = as_strided(py[2*maxlag:], shape=(2*maxlag+1, len(y) + 2*maxlag),\n",
    "                   strides=(-py.strides[0], py.strides[0]))\n",
    "    px = np.pad(x, maxlag, mode='constant')\n",
    "\n",
    "    \n",
    "    cross_corr = T.dot(px)\n",
    "    \n",
    "    '''if normalize:\n",
    "        # Subtract means\n",
    "        mx = np.mean(x)\n",
    "        my = np.mean(y)\n",
    "        norm = float(len(y) * mx * my)\n",
    "\n",
    "        # Compute the numerator as the sum of products of differences from means\n",
    "        numerator = cross_corr - norm\n",
    "\n",
    "        # Compute the denominator\n",
    "        sigma_x = np.sqrt(np.sum((x - mx) ** 2))\n",
    "        sigma_y = np.sqrt(np.sum((y - my) ** 2))\n",
    "        denominator = float(sigma_x * sigma_y)\n",
    "        \n",
    "        # Normalize the cross-correlation\n",
    "        cross_corr = numerator / denominator'''\n",
    "\n",
    "    cmax = float(cross_corr.max())\n",
    "    the_lagmax = cross_corr.argmax() - (max_lag + 1)\n",
    "    \n",
    "    if normalize:\n",
    "        mx = np.mean(x)\n",
    "        my = np.mean(y)\n",
    "        norm = float(len(y) * mx * my)\n",
    "\n",
    "        \n",
    "        numerator = cmax - norm\n",
    "\n",
    "        sigma_x = np.sqrt(np.sum((x - mx) ** 2))\n",
    "        sigma_y = np.sqrt(np.sum((y - my) ** 2))\n",
    "        denominator = float(sigma_x * sigma_y)\n",
    "        \n",
    "        cmax = numerator / denominator\n",
    "\n",
    "    return cmax, the_lagmax\n",
    "\n",
    "\n",
    "def analyze_chunk(data_chunk, index, nodes, max_lag, dec):\n",
    "    numpy_array = np.zeros((37*72, 37*72))  \n",
    "    if index ==0:\n",
    "        timelag_array = np.zeros((37*72, 37*72))\n",
    "    \n",
    "    for indi, nod in enumerate(nodes):\n",
    "        Ai, Aj = nodes[indi]\n",
    "        for indj, node in enumerate(nodes):\n",
    "            (Bi, Bj) = nodes[indj]\n",
    "\n",
    "            if indi < indj:\n",
    "                crossmax, the_lagmax = crosscorrelation(data_chunk.values[:,Ai, Aj], data_chunk.values[:,Bi, Bj], max_lag)\n",
    "\n",
    "                # Undirected Network\n",
    "                #crossmax = cross_corr.max()\n",
    "                \n",
    "\n",
    "                numpy_array[indi, indj] = crossmax\n",
    "                if index == 0:\n",
    "                    timelag_array[indi, indj] = the_lagmax\n",
    "\n",
    "\n",
    "                '''  # Directed Network\n",
    "                first_half = cross_corr[:max_lag + 1]\n",
    "                max_first_half = np.max(first_half)\n",
    "                second_half = cross_corr[max_lag:]\n",
    "                max_second_half = np.max(second_half)\n",
    "                numpy_array[indi, indj] = max_first_half                \n",
    "                numpy_array[indj, indi] = max_second_half   \n",
    "                \n",
    "                if index ==0:\n",
    "                    lag_first_half = np.argmax(first_half) - max_lag\n",
    "                    lag_second_half = np.argmax(second_half)\n",
    "                    timelag_array[indi, indj] = lag_first_half\n",
    "                    timelag_array[indj, indi] = lag_second_half\n",
    "                \n",
    "                '''\n",
    "\n",
    "    np.save(f'X:/cross correlation max 2022 2100 low emission/corrs_{index}_year_{dec}.npy', numpy_array)\n",
    "    if index == 0:\n",
    "        np.save(f'X:/Timelag 2022 2100 low emission/timelag_{index}_year_{dec}.npy', timelag_array)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # DATA OUTPUT\n",
    "    foutpath = \"X:/cross correlation max 2022 2100 low emission\"\n",
    "    if not os.path.exists(foutpath):\n",
    "        os.makedirs(foutpath)\n",
    "    foutpath2 = \"X:/Timelag 2022 2100 low emission\"\n",
    "    if not os.path.exists(foutpath2):\n",
    "        os.makedirs(foutpath2)\n",
    "\n",
    "    # PARAMETERS\n",
    "\n",
    "    max_lag = 30*5           #five months\n",
    "    num_surrogates = 30\n",
    "    start_year = 2022\n",
    "    end_year = 2100\n",
    "\n",
    "    start_days = []\n",
    "    current_day = 0  # 1 jan 2022\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        \n",
    "        start_days.append(current_day)\n",
    "        if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
    "            days_in_year = 366\n",
    "        else:\n",
    "            days_in_year = 365\n",
    "        current_day += days_in_year \n",
    "    last_day_index = 28854\n",
    "    start_days.append(28854) \n",
    "\n",
    "    # Caricare il dataset NetCDF\n",
    "    ds = xr.open_dataset('X:\\surr_IAAFT_t2m_2022_2100_lowemission.nc', chunks={'num': 1})\n",
    "    #print(ds)\n",
    "    sh_t2m = ds['t2m']\n",
    "    lon = ds['lon']\n",
    "    lat = ds['lat']\n",
    "    #print(lon)\n",
    "    #print(lat)\n",
    "\n",
    "\n",
    "    lon_range = range(0,len(lon))\n",
    "    lat_range = range(0,len(lat))\n",
    "    nodes = tuple((i,j) for i in lat_range for j in lon_range)\n",
    "\n",
    "\n",
    "    # DASK parallelation\n",
    "    \n",
    "    delayed_results = []\n",
    "\n",
    "    for i in range(num_surrogates+1):\n",
    "        for dec in range(len(start_days) - 1):  # -1 perché l'ultimo elemento in start_days non ha un periodo successivo\n",
    "            data_chunk = sh_t2m.isel(num=i, time=slice(start_days[dec], start_days[dec+1]))\n",
    "            delayed_results.append(dask.delayed(analyze_chunk)(data_chunk, i, nodes, max_lag, dec))\n",
    "    dask.visualize(*delayed_results, optimize_graph=True, filename='my_graph.svg')\n",
    "    \n",
    "    \n",
    "    # Calcolare i risultati\n",
    "    computed_results = dask.compute(*delayed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    radius = 6371 # average radius\n",
    "\n",
    "    # degree to radiant\n",
    "    lat1_rad = lat1 * math.pi / 180\n",
    "    lon1_rad = lon1 * math.pi / 180\n",
    "    lat2_rad = lat2 * math.pi / 180\n",
    "    lon2_rad = lon2 * math.pi / 180\n",
    "\n",
    "    d_lat = lat2_rad - lat1_rad\n",
    "    d_lon = lon2_rad - lon1_rad\n",
    "\n",
    "    # haversine formula \n",
    "    a = math.sin(d_lat / 2) ** 2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(d_lon / 2) ** 2\n",
    "    if a > 1:\n",
    "        a = 1.0\n",
    "        \n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = radius * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "# NetCDF\n",
    "ds = xr.open_dataset('X:\\surr_IAAFT_t2m_2022_2100_lowemission.nc', decode_times=False)  # Change dataset \n",
    "sh_t2m = ds['t2m']\n",
    "lon = ds['lon']\n",
    "lat = ds['lat']\n",
    "print(lon)\n",
    "print(lat)\n",
    "\n",
    "lon_range = range(0,len(lon))\n",
    "lat_range = range(0,len(lat))\n",
    "nodes = tuple((i,j) for i in lat_range for j in lon_range)\n",
    "\n",
    "distance_matrix = np.zeros((len(nodes), len(nodes)))\n",
    "\n",
    "for i, (lat1_index, lon1_index) in enumerate(nodes):\n",
    "    #print(i)\n",
    "    for j, (lat2_index, lon2_index) in enumerate(nodes):\n",
    "        print(i, '   ', j)\n",
    "        if j >= i:  # Questo assicura che calcoliamo solo la metà superiore\n",
    "            lat1 = lat[lat1_index].item()  # Convert to Python scalar if it's an xarray DataArray\n",
    "            lon1 = lon[lon1_index].item()\n",
    "            lat2 = lat[lat2_index].item()\n",
    "            lon2 = lon[lon2_index].item()\n",
    "            \n",
    "            distance = haversine_distance(lat1, lon1, lat2, lon2)\n",
    "            distance_matrix[i, j] = distance\n",
    "            distance_matrix[j, i] = distance  # Copia il valore nella metà opposta\n",
    "\n",
    "#np.save('distance_matrix_grid5_70_22.npy', distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.special import erfc\n",
    "\n",
    "def probabilityfrom(Z: np.ndarray, dis: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate probability based on the values of Z and dis.\n",
    "    \n",
    "    Parameters:\n",
    "        Z (np.ndarray): A parameter of the function.\n",
    "        dis (np.ndarray): Another parameter of the function.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The calculated probability.\n",
    "    \"\"\"\n",
    "    e = np.e\n",
    "    \n",
    "    # Calculate pval based on the value of Z\n",
    "    pval = erfc(Z)\n",
    "    # Compute B_value using the calculated pval\n",
    "    B_value = np.where(pval < e**(-1), -e*pval*np.log(np.abs(pval)), 1)\n",
    "    \n",
    "    # Compute prior and prob values\n",
    "    prior = np.exp(-dis/2000)\n",
    "    prob = 1 - (1 + ((B_value) * (1-prior) / (prior))**(-1))**(-1)\n",
    "\n",
    "    return prob\n",
    "\n",
    "# Parameters\n",
    "surrogates = 30\n",
    "years = 79\n",
    "distance_matrix = np.load('X:/distance_matrix_corrected_CMIP6.npy')\n",
    "\n",
    "# Carica gli array numpy e aggiungili alla lista\n",
    "inpath = \"X:/cross correlation max 2022 2100 high emission\"\n",
    "outpath = \"X:/Fuzzy Networks IAAFT CMIP6 high emission\" \n",
    "if not os.path.exists(outpath):\n",
    "    os.makedirs(outpath)\n",
    "\n",
    "outpath2 = \"X:/Zscores IAAFT CMIP6 high emission\" \n",
    "if not os.path.exists(outpath2):\n",
    "    os.makedirs(outpath2)\n",
    "    \n",
    "for yr in range(years):\n",
    "    arrays = []\n",
    "    for i in range(0, surrogates+1):\n",
    "        filename = f'{inpath}/corrs_{i}_year_{yr}.npy'\n",
    "        # Controlla se il file esiste\n",
    "        if not os.path.exists(filename):\n",
    "            print(f'File non trovato: {filename}, continua con il prossimo.')\n",
    "            continue\n",
    "        loaded_array = np.load(filename)\n",
    "        arrays.append(loaded_array)\n",
    "    # Unisci gli array lungo un nuovo asse\n",
    "    result_array = np.stack(arrays, axis=0)\n",
    "    \n",
    "    # Calcola la media e la deviazione standard delle altre correlazioni (1 fino a 10) per ogni periodo e nodoA\n",
    "    medie = result_array[1:, :, :].mean(axis=0)\n",
    "    deviazioni_standard = result_array[1:, :, :].std(axis=0)\n",
    "\n",
    "    # Calcola lo z-score per tutte le correlazioni utilizzando le medie e le deviazioni standard calcolate\n",
    "    z_scores = abs(result_array[0, :, :] - medie) / deviazioni_standard\n",
    "\n",
    "    print(z_scores)\n",
    "    np.save(f'{outpath2}/zscores_npy_year{2022+yr}.npy', z_scores)\n",
    "    prob_array = probabilityfrom(z_scores, distance_matrix)\n",
    "    print(prob_array)\n",
    "    np.save(f'{outpath}/prob_npy_year{2022+yr}.npy', prob_array)\n",
    "\n",
    "    print('year: ', 2022+yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# Lettura del dataset originale con chunk adeguati\n",
    "dataset_path = 'X:/surr_IAAFT_t2m_2022_2100_highemission.nc'\n",
    "original_ds = xr.open_dataset(dataset_path, chunks={'time': 'auto'})\n",
    "\n",
    "# Definisci i giorni per ciascun anno (2022-2037)\n",
    "start_year = 2022\n",
    "end_year = 2022 + 16 - 1  # 2022 + 15 = 2037\n",
    "start_date = f'{start_year}-01-01'\n",
    "end_date = f'{end_year}-12-31'\n",
    "\n",
    "# Seleziona i primi 16 anni\n",
    "subset_ds = original_ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "# Salva il nuovo dataset su disco\n",
    "subset_dataset_path = 'X:/surr_IAAFT_t2m_2022_2037_highemission.nc'\n",
    "subset_ds.to_netcdf(subset_dataset_path)\n",
    "\n",
    "print(f\"Nuovo dataset salvato in: {subset_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debug new method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask\n",
    "from dask.distributed import LocalCluster, Client\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configurazione del logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Funzione per calcolare i giorni per ogni anno\n",
    "def days_per_year(start_year, end_year):\n",
    "    days = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
    "            days.append(366)\n",
    "        else:\n",
    "            days.append(365)\n",
    "    return days\n",
    "\n",
    "# Calcolo dei giorni per ogni anno\n",
    "start_year = 2022\n",
    "#end_year = 2100\n",
    "end_year = 2022 + 16 - 1\n",
    "days = days_per_year(start_year, end_year)\n",
    "\n",
    "# Definire i chunk per il dataset\n",
    "def create_chunks(days):\n",
    "    return tuple(days)\n",
    "\n",
    "# Lettura del dataset NetCDF con chunk adeguati\n",
    "dataset_path = 'X:/surr_IAAFT_t2m_2022_2037_highemission.nc'\n",
    "original_ds = xr.open_dataset(dataset_path, chunks={'time': create_chunks(days)})\n",
    "\n",
    "logger.info(original_ds)\n",
    "# Parametri\n",
    "max_lag = 150\n",
    "outpath = \"X:/zscore_t2m_2022_2100_highemission\"\n",
    "if not os.path.exists(outpath):\n",
    "    os.makedirs(outpath)\n",
    "\n",
    "lon_range = range(0, len(original_ds['lon']))\n",
    "lat_range = range(0, len(original_ds['lat']))\n",
    "nodes = tuple((i,j) for i in lat_range for j in lon_range)\n",
    "\n",
    "# Salva la lista dei nodi\n",
    "with open(f\"{outpath}/nodelist.txt\", \"w\") as f:\n",
    "    for idx, (lat_idx, lon_idx) in enumerate(nodes):\n",
    "        lat = original_ds['lat'].values[lat_idx]\n",
    "        lon = original_ds['lon'].values[lon_idx]\n",
    "        f.write(f\"{idx} {lat} {lon}\\n\")\n",
    "\n",
    "# Funzione di cross-correlazione\n",
    "def crosscorrelation(x, y, maxlag, normalize=True):\n",
    "    py = np.pad(y.conj(), 2*maxlag, mode='constant')\n",
    "    T = as_strided(py[2*maxlag:], shape=(2*maxlag+1, len(y) + 2*maxlag),\n",
    "                   strides=(-py.strides[0], py.strides[0]))\n",
    "    px = np.pad(x, maxlag, mode='constant')\n",
    "    cross_corr = T.dot(px)\n",
    "    cmax = float(cross_corr.max())\n",
    "    the_lagmax = cross_corr.argmax() - (maxlag + 1)\n",
    "    \n",
    "    if normalize:\n",
    "        mx = np.mean(x)\n",
    "        my = np.mean(y)\n",
    "        norm = float(len(y) * mx * my)\n",
    "        numerator = cmax - norm\n",
    "        sigma_x = np.sqrt(np.sum((x - mx) ** 2))\n",
    "        sigma_y = np.sqrt(np.sum((y - my) ** 2))\n",
    "        denominator = float(sigma_x * sigma_y)\n",
    "        cmax = numerator / denominator\n",
    "    return cmax, the_lagmax\n",
    "\n",
    "# Funzione di analisi dei chunk\n",
    "def analyze_chunk(data_chunk_subset, nodes, max_lag, year, outpath):\n",
    "    logger.info(f\"Processing year {year} with {len(nodes)} nodes\")\n",
    "    \n",
    "    arrays = []\n",
    "    \n",
    "    for i in range(data_chunk_subset.dims['num']):\n",
    "        num_chunks = data_chunk_subset['t2m'].isel(num=i)\n",
    "        numpy_array = np.zeros((37*72, 37*72))  \n",
    "        \n",
    "        for indi, nod in enumerate(nodes):\n",
    "            Ai, Aj = nodes[indi]\n",
    "            for indj, node in enumerate(nodes):\n",
    "                (Bi, Bj) = nodes[indj]\n",
    "\n",
    "                if indi < indj:\n",
    "                    ts1 = num_chunks.isel(lat=Ai, lon=Aj).values\n",
    "                    ts2 = num_chunks.isel(lat=Bi, lon=Bj).values\n",
    "                    cmax, the_lagmax = crosscorrelation(ts1, ts2, max_lag)\n",
    "                    numpy_array[indi, indj] = cmax\n",
    "\n",
    "        arrays.append(numpy_array)\n",
    "\n",
    "    result_array = np.stack(arrays, axis=0)\n",
    "     \n",
    "    medie = result_array[1:, :, :].mean(axis=0)\n",
    "    deviazioni_standard = result_array[1:, :, :].std(axis=0)\n",
    "   \n",
    "    z_scores = abs(result_array[0, :, :] - medie) / deviazioni_standard\n",
    "    np.save(f'{outpath}/zscores_npy_year{year}.npy', z_scores)\n",
    "    logger.info(f\"Finished processing year {year}\")\n",
    "\n",
    "# Configurazione del cluster Dask\n",
    "if __name__ == \"__main__\":\n",
    "    cluster = LocalCluster(n_workers=8)\n",
    "    client = Client(cluster)\n",
    "    logger.info(f\"Dask Dashboard: {client.dashboard_link}\")\n",
    "\n",
    "    grouped_ds = original_ds.groupby('time.year')\n",
    "\n",
    "    delayed_results = []\n",
    "\n",
    "    for year, data_chunk in grouped_ds:\n",
    "        data_chunk_subset = data_chunk.isel(num=slice(0, 31))\n",
    "        delayed_results.append(dask.delayed(analyze_chunk)(data_chunk_subset, nodes, max_lag, year, outpath))\n",
    "\n",
    "    dask.visualize(*delayed_results, optimize_graph=True, filename='my_graph.svg')\n",
    "    graph_path = os.path.abspath('my_graph.svg')\n",
    "    import webbrowser\n",
    "    webbrowser.open(f'file://{graph_path}')\n",
    "\n",
    "    computed_results = dask.compute(*delayed_results)\n",
    "    logger.info(\"Completed all tasks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
